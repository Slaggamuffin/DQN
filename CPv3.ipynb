{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import csv\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    ''' Deep Q Neural Network class. '''\n",
    "    def __init__(self,env_name, episodes):\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "        #self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        #Replay memory\n",
    "        self.replay_memory = []\n",
    "        self.replay_size = 128\n",
    "        self.capacity = 10000\n",
    "        \n",
    "        #State Variables\n",
    "        self.ROWS = 160\n",
    "        self.COLS = 240\n",
    "        self.REM_STEP = 4\n",
    "        self.EPISODES = episodes\n",
    "        self.image_memory = np.zeros((self.REM_STEP,self.ROWS, self.COLS))\n",
    "        \n",
    "        #Greedy Variables\n",
    "        self.epsilon = 0.95\n",
    "        self.eps_decay = 0.99\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Model variables    \n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.linear_input=147264\n",
    "        #self.linear_input = self.out_rows*self.out_cols*16      \n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d(\n",
    "                            in_channels=self.REM_STEP\n",
    "                            ,out_channels=64\n",
    "                            ,kernel_size=4\n",
    "                            ,stride=2\n",
    "                        ),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Conv2d(\n",
    "                            in_channels=64\n",
    "                            ,out_channels=64\n",
    "                            ,kernel_size=3\n",
    "                            ,stride=2\n",
    "                            ),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Conv2d(\n",
    "                            in_channels=64\n",
    "                            ,out_channels=32\n",
    "                            ,kernel_size=3\n",
    "                            ,stride=2\n",
    "                            ),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Flatten(),\n",
    "                        torch.nn.Linear(17632,512),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(512,128),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(128,self.action_dim)          \n",
    "                    )\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),0.00025)\n",
    "        \n",
    "        self.target = copy.deepcopy(self.model)\n",
    "        self.target_update_interval = 10\n",
    "         \n",
    "        \n",
    "    # Model functions  \n",
    "    def update(self, states, targets):\n",
    "        \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
    "        y_pred = self.model(torch.Tensor(states).to(self.device))\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(targets).to(self.device)))\n",
    "        print(loss)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "\n",
    "    def predict(self,network, state):\n",
    "        \"\"\" Compute Q values for all actions using the DQL. \"\"\"\n",
    "        if network == 'model':\n",
    "            with torch.no_grad():\n",
    "                return self.model(torch.Tensor(state).to(self.device))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return self.target(torch.Tensor(state).to(self.device))\n",
    "\n",
    "        \n",
    "    def replay(self):\n",
    "        ''' Add experience replay to the DQL network class.'''\n",
    "        if len(self.replay_memory) >= self.replay_size:\n",
    "            # Sample experiences from the agent's memory\n",
    "            data = random.sample(self.replay_memory, self.replay_size)\n",
    "            states = []\n",
    "            targets = []\n",
    "            for state, action, next_state, reward, done in data:\n",
    "                states.append(state.squeeze(0))\n",
    "                q_values = self.predict('model',state).squeeze(0)\n",
    "                if done:\n",
    "                    q_values[action] = reward\n",
    "                else:\n",
    "                    q_values_next = self.predict('target',next_state)\n",
    "                    q_values[action] = reward + self.gamma * torch.max(q_values_next).item()\n",
    "\n",
    "                targets.append(q_values.tolist())\n",
    "            \n",
    "            self.update(states, targets)\n",
    "            \n",
    "    def memory_append(self,memory):\n",
    "        \"\"\" Adds a maximum capacity to replay memory \"\"\"\n",
    "        count = 0\n",
    "        \n",
    "        if len(self.replay_memory) < self.capacity:\n",
    "            self.replay_memory.append(memory)\n",
    "        else:\n",
    "            self.replay_memory[count % self.capacity] = memory #if replay memory is over capacity will start to re-write\n",
    "        count += 1\n",
    "            \n",
    "            \n",
    "            \n",
    " # State functions\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(env_name+str(rem_step), image[rem_step,...])\n",
    "        cv2.waitKey(200)\n",
    "        cv2.destroyAllWindows()\n",
    "        return\n",
    "            \n",
    "       \n",
    "    def GetImage(self):\n",
    "        img = self.env.render(mode='rgb_array')\n",
    "  \n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img_rgb_resized = cv2.resize(img_rgb, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "        img_rgb_resized[img_rgb_resized < 255] = 0\n",
    "        img_rgb_resized = img_rgb_resized / 255\n",
    "\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "        self.image_memory[0,:,:] = img_rgb_resized\n",
    "    \n",
    "        #self.imshow(self.image_memory,0)\n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage()\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def step(self,action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = self.GetImage()\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "# Action function\n",
    "    def act(self,state):\n",
    "        q_values = self.predict('model',state)\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = torch.argmax(q_values).item()\n",
    "        return action\n",
    "    \n",
    "# Main loop\n",
    "    def run(self):\n",
    "        final = []\n",
    "        for episode in range(self.EPISODES):\n",
    "            # Get_state\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            count = 0\n",
    "            sum_total_replay_time = 0\n",
    "            if episode % self.target_update_interval == 0:\n",
    "                self.update_target()\n",
    "            while not done:               \n",
    "                # Select action via Greedy_strategy\n",
    "                action = self.act(state)\n",
    "                # Get next_state\n",
    "                next_state, reward, done, info = self.step(action)\n",
    "                self.memory_append((state, action, next_state, reward, done))\n",
    "                # predict and target q_values and update them\n",
    "                t0=time.time()\n",
    "                self.replay()\n",
    "                t1=time.time()\n",
    "                sum_total_replay_time+=(t1-t0)\n",
    "                count += 1\n",
    "            \n",
    "                if done:\n",
    "                    final.append(count)\n",
    "                    break\n",
    "                    \n",
    "                state = next_state    \n",
    "                \n",
    "            #update epsilon        \n",
    "            self.epsilon = max(self.epsilon * self.eps_decay, 0.01)\n",
    "            print(\"episode: {}, total reward: {}\".format(episode, count))\n",
    "            print(\"replay time: {}\".format(sum_total_replay_time/count))\n",
    "            clear_output(wait=True)\n",
    "        self.env.close()  \n",
    "        return final\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total reward: 18\n",
      "replay time: 5.560451083713108e-05\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_name = 'CartPole-v1'\n",
    "    agent = DQN(env_name,1)\n",
    "    episodes = agent.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.model, 'DoubleDQN_ep300_lr000025')\n",
    "np.savetxt(\"DoubleDQN_300ep.csv\",np.array(episodes),delimiter=',',fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_size(w,k,s):\n",
    "    return (w-k)/s+1\n",
    "    \n",
    "\n",
    "a = conv_output_size(160,4,2)\n",
    "b = conv_output_size(a,3,2)\n",
    "c = conv_output_size(b,3,2)\n",
    "\n",
    "d = conv_output_size(240,4,2)\n",
    "e = conv_output_size(d,3,2)\n",
    "f = conv_output_size(e,3,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
