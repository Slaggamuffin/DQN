{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cart and Pole attempt 2\n",
    "\n",
    "There are 2 ways to solve the Cart and Pole env\n",
    "\n",
    "1.) problem specific solution:\n",
    "    takes specific observations about the environment i.e pole velocity, pole angle, cart position, cart velocity. \n",
    "    Take these observations to represent state and calculate Q-values accordingly\n",
    "\n",
    "\n",
    "\n",
    "2.) Generalised solution:\n",
    "    takes in simply the pixels of the screen as inputs. (Deepmind took 4 consecutive images as inputs (this is so the AI has a temporal understanding of the env)\n",
    "    Screen inputs = state which is used to estimate Q-values\n",
    "    \n",
    "\n",
    "Bellman Equation Derivation\n",
    "\n",
    "Q(st,at) = Rt+1 + yRt+2 + y2Rt+3 \n",
    "\n",
    "Q value for a state action pair is given by the sum of all future rewards in subsequent states. We are less certain about rewards in future states so  a discount rate applied\n",
    "\n",
    "Q(st+1,at+1) = Rt+2 + yRt+3 + y2Rt+4\n",
    "\n",
    "As you can see the Q values for the next state will be represent as such. Therfore Q(st,at) can actually be represented the following way:\n",
    "\n",
    "Q(st,at) = Rt+1 + y * Q(st+1,at+1)\n",
    "\n",
    "This can be simplified further. The action in t+1 will be the best action for its given state-action pair. \n",
    "\n",
    "at+1 = at+1 : Q(st+1,at+1) = max(Q(st+1))\n",
    "\n",
    "Therefore, our final equation is:\n",
    "\n",
    "Q(s,a) = Rt+1 + y * max(Qst+1)\n",
    "\n",
    "\n",
    "Learning Q values\n",
    "\n",
    "We want all values in our Q table to satisfy the bellman equation. Since we do not immediately know the Q values they must be learned via iterations through small updates as we lean the environment\n",
    "\n",
    "q(s,a) = q(s,a) + a(target - q(s,a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Discrete(16)\n",
      "Action space: Discrete(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_pole = \"CartPole-v1\"\n",
    "mountain_car = \"MountainCar-v0\"\n",
    "mountain_car_cont = \"MountainCarContinuous-v0\"\n",
    "acrobot = \"Acrobot-v1\"\n",
    "pendulum = \"Pendulum-v0\"\n",
    "frozen_lake = \"FrozenLake-v0\"\n",
    "env = gym.make(frozen_lake)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = \\\n",
    "            type(env.action_space) == gym.spaces.discrete.Discrete\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n\n",
    "            print(\"Action size:\", self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "            print(\"Action range:\", self.action_low, self.action_high)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low,\n",
    "                                       self.action_high,\n",
    "                                       self.action_shape)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(Agent):\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 discount_rate =0.99,\n",
    "                 learning_rate =0.1,\n",
    "                 min_exploration_rate =0.01,\n",
    "                 max_exploration_rate =1,\n",
    "                 exploration_decay_rate= 0.001,   \n",
    "                ):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n\n",
    "        print(\"state size:\", self.state_size)\n",
    "        \n",
    "        self.eps = 1\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.max_exploration_rate = max_exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.q_table = np.zeros([self.state_size,self.action_size])\n",
    "        \n",
    "    def get_action(self,state):\n",
    "        q_state = self.q_table[state]\n",
    "        exploit = np.argmax(q_state)\n",
    "        explore = super().get_action(state)\n",
    "        return explore if np.random.random() < self.eps else exploit\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_old = self.q_table[state,action]\n",
    "        q_next = self.q_table[next_state]\n",
    "        q_next = np.zeros([self.action_size]) if done else q_next\n",
    "\n",
    "        self.q_table[state,action] = (1- self.learning_rate) * q_old \\\n",
    "        + self.learning_rate *(reward + self.discount_rate * np.max(q_next))\n",
    "\n",
    "    def decay(self,episode):\n",
    "        self.eps = self.min_exploration_rate + \\\n",
    "        (self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate*episode)\n",
    "        return self.eps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3fc7640283aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'QAgent' is not defined"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)       \n",
    "num_episodes = 400\n",
    "\n",
    "\n",
    "total_reward = 0\n",
    "for ep in range(num_episodes):\n",
    "    if ep % 1000 == 0:\n",
    "        print(ep)\n",
    "        print(total_reward)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    for step in range(100):\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.train((state, action, next_state, reward, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            agent.decay(ep)\n",
    "            break\n",
    "        \n",
    "print(total_reward)   \n",
    "print(agent.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4909542  0.45334818 0.46546493 0.45782367]\n",
      " [0.30429607 0.28255306 0.22845774 0.39051919]\n",
      " [0.29439742 0.25832868 0.25530994 0.26097333]\n",
      " [0.03822265 0.14998099 0.05042876 0.07943631]\n",
      " [0.52393394 0.31258843 0.32656837 0.38475094]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.22230575 0.15603042 0.15714952 0.15868687]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32023891 0.3883467  0.33936479 0.56698469]\n",
      " [0.47753114 0.64420596 0.34044153 0.37090803]\n",
      " [0.57198816 0.29637058 0.4192104  0.2465289 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44327615 0.47935633 0.75870145 0.46250383]\n",
      " [0.69563635 0.90033893 0.73898216 0.80188869]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(agent.q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
