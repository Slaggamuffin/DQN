{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Reinforcement Learning - Frozen Lake\n",
    "\n",
    "Frozen lake, you have to navigate to goal G. You cannot step on H's which represent holes. You must move on Fs which represent frozen bits of lake\n",
    "\n",
    "The board looks like this:\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "\n",
    "Therefore there are 16 possible states. And for each state there are 4 possible actions: up, down, left and right. Our Q table is an array of size (16,4) \n",
    "\n",
    "Implementing the q-learning algorithm\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    initialise new episode parameters\n",
    "    \n",
    "    for step in range(max_stepts_per_episode):\n",
    "        exploration-exploitation trade-off\n",
    "        take new action\n",
    "        update q-table\n",
    "        set new state\n",
    "        add new reward\n",
    "       \n",
    "    exploration rate decay\n",
    "    Add current episode reward to total rewards list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size,action_space_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate =0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "epoch : 1000\n",
      "epoch : 2000\n",
      "epoch : 3000\n",
      "epoch : 4000\n",
      "epoch : 5000\n",
      "epoch : 6000\n",
      "epoch : 7000\n",
      "epoch : 8000\n",
      "epoch : 9000\n",
      "[[0.53008463 0.49651265 0.49880147 0.48127751]\n",
      " [0.40704137 0.29077209 0.32136697 0.47189096]\n",
      " [0.40662012 0.4030405  0.3962986  0.45487721]\n",
      " [0.25956898 0.39598715 0.29827863 0.44267543]\n",
      " [0.55520866 0.37117075 0.26840244 0.32311451]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.17208172 0.15710682 0.41536734 0.15936024]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44205711 0.47643621 0.41299303 0.60353738]\n",
      " [0.39372986 0.70244506 0.45191555 0.47526986]\n",
      " [0.75511526 0.39777382 0.26101322 0.37438445]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33922827 0.401842   0.79915261 0.45566329]\n",
      " [0.67933998 0.88682921 0.7430845  0.7310592 ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "0.010044990898875783\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    if episode % 1000 == 0:\n",
    "        print('epoch :', len(rewards_all_episodes))\n",
    "    for step in range(max_steps_per_episode): \n",
    "\n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate: # Exploit\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample() # Explore\n",
    "            \n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # Get new state and reward for action\n",
    "        \n",
    "        # Update Q_table\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "        learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "        if done == True: \n",
    "            break\n",
    "    \n",
    "    # Exploration decay function\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "print(q_table)\n",
    "print(exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "fell through hole\n"
     ]
    }
   ],
   "source": [
    "# Visualisation\n",
    "\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\" Episode \", episode + 1,)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"win\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"fell through hole\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5191.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rewards_all_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
